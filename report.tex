%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}


\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{Machine learning for Data science 1}

% Interim or final report
\Archive{HW6 report} 
%\Archive{Final report} 

% Article title
\PaperTitle{HW6 report} 

% Authors (student competitors) and their info
\Authors{Maša Kljun, 63170011}

% Advisors
\affiliation{\textit{}}

% Keywords
\Keywords{Artificial Neural Network, classification, regression ...}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{ 
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Results}
\subsection*{Task 1}
We implement this problem in \texttt{hw6.py} and implement it in a way that classification and regression ANNs share a lot of code. The only difference between the models is in the activation functions of the last layer and in the loss functions. In classification there are \textit{n\_unique} output neurons, where \textit{n\_unique} is the number of unique classes in the target variable, while in regression there is only one neuron in the output layer. The activation of the last layer in classification is the softmax function, which enables us to get probabilities of each class, while the activation of the last layer in regression is the identity function. As mentioned before, the second difference is in the loss functions. In classification we use cross entropy, while in regression we use mean squared error. Most of the other code (computing gradient, etc.) is shared among both models.

\subsection*{Task 2}
We verify the gradient with the help of the following definition of the derivative:
\begin{equation}
\label{eq1}
f'(a) = \lim_{h \rightarrow 0} \frac{f(a+h) - f(a)}{h}.
\end{equation}
We loop through the whole vector of weights and biases $wb$ and in each iteration obtain vector $\bar{wb}^{(i)}$, where the $i$-th component of vector $wb$ is changed by adding a small value $h = 1e-06$, while leaving other components of the vector as they are. We then use the above Equation \ref{eq1} and compute the $i$-th component of the estimated gradient:
\begin{equation}
\nabla f^{(i)}(wb) = \frac{f(\bar{wb}^{(i)})-f(wb)}{h}.
\end{equation}

We compare the obtained component $\nabla f^{(i)}(wb)$ with the true value of derivative obtained by the backpropagation function $\nabla f^{(i)}_{bp}(wb)$. If the absolute difference between $\nabla f^{(i)}(wb)$ and $\nabla f^{(i)}_{bp}(wb)$ is smaller than predefined threshold $thresh = 1e-02$ we conclude the gradient for $i$-th component is correct. If gradient for all components is correct, we conclude the gradient is correct. We verify the gradient on the initial set of weights everytime we fit the model.

\subsection*{Task 3}
We apply \texttt{housing2r} and \texttt{housing3} data sets to the regression and the classification models, respectively. We use $5$-fold cross-validation for both data sets in order to find the best values for regularization parameter $\lambda$ and the number of hidden layers $units$. We choose some options for these two parameters manually, by checking which settings work, and then apply $5$-fold cross-validation. We perform training and optimal parameter search on $80 \%$ of the data and then evaluate the model on the remaining $20\%$ of the data. Also note that we shuffle the data before $80/20$ split and scale them after using StandardScaler. 

For the regression model we see from the CV that the best setting is $units: [10]$ (one hidden layer with 10 nodes) and $\lambda = 0.01$. We then evaluate the obtained model with the test set and get $MSE = 0.53$. We compare the obtained results with the Support Vector Regression (SVR) with RBF kernel and the following settings: $\sigma = 5, \lambda = 1, \epsilon = 1$. 



%------------------------------------------------

\section*{Discussion}



%------------------------------------------------


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
%\bibliographystyle{unsrt}
%\bibliography{report}


\end{document}